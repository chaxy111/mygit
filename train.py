import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_scheduler
from datasets import load_from_disk
from torch.utils.data import DataLoader
from tqdm import tqdm
import os
import numpy as np

# === å‚æ•°è®¾ç½® ===
model_name_or_path = "./model"  # è¯·ç¡®ä¿è·¯å¾„ä¸‹æœ‰ config.jsonã€pytorch_model.binã€tokenizer
batch_size = 4
num_epochs = 10
max_input_length = 128
max_target_length = 256
save_path = "./output"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ å½“å‰è®¾å¤‡ï¼š{device}")

# === åŠ è½½æ¨¡å‹å’Œ tokenizer ===
tokenizer = T5Tokenizer.from_pretrained(model_name_or_path)
model = T5ForConditionalGeneration.from_pretrained(model_name_or_path).to(device)

# === åŠ è½½æ•°æ®é›† ===
dataset = load_from_disk("preprocessed_data")

def preprocess_function(examples):
    inputs = tokenizer(
        examples["input"],
        max_length=max_input_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    targets = tokenizer(
        examples["target"],
        max_length=max_target_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    inputs["labels"] = targets["input_ids"]
    return inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)
tokenized_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)
train_dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)

# === ä¼˜åŒ–å™¨ä¸ scheduler ===
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_dataloader) * num_epochs
num_warmup_steps = int(0.1 * num_training_steps)

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)

# === æ··åˆç²¾åº¦è®­ç»ƒç›¸å…³ ===
scaler = torch.cuda.amp.GradScaler()

# === åˆ›å»ºè¾“å‡ºç›®å½• ===
os.makedirs(save_path, exist_ok=True)

# === è¯„ä¼°å‡½æ•° ===
def compute_metrics(model, tokenizer, dataloader, device):
    model.eval()
    match_count = 0
    total_count = 0
    total_precision = []

    for batch in tqdm(dataloader, desc="ğŸ” æ­£åœ¨è¯„ä¼°"):
        inputs = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        with torch.no_grad():
            generated_ids = model.generate(
                input_ids=inputs,
                attention_mask=attention_mask,
                max_length=max_target_length,
                num_beams=4
            )

        preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        targets = tokenizer.batch_decode(labels, skip_special_tokens=True)

        for pred, target in zip(preds, targets):
            pred = pred.strip()
            target = target.strip()

            total_count += 1
            if pred == target:
                match_count += 1

            pred_set = set([x.strip() for x in pred.split("[SEP]") if x.strip()])
            target_set = set([x.strip() for x in target.split("[SEP]") if x.strip()])

            if len(pred_set) == 0:
                total_precision.append(0)
            else:
                correct = len(pred_set & target_set)
                precision = correct / len(pred_set)
                total_precision.append(precision)

    accuracy = match_count / total_count
    precision = np.mean(total_precision)
    return accuracy, precision

# === å¼€å§‹è®­ç»ƒ ===
for epoch in range(num_epochs):
    print(f"\nğŸŒ€ Epoch {epoch + 1}/{num_epochs}")
    model.train()
    total_loss = 0.0

    train_iter = tqdm(train_dataloader, desc=f"Training Epoch {epoch + 1}")
    for step, batch in enumerate(train_iter):
        batch = {k: v.to(device) for k, v in batch.items()}
        optimizer.zero_grad()

        with torch.cuda.amp.autocast():
            outputs = model(**batch)
            loss = outputs.loss

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()
        lr_scheduler.step()

        total_loss += loss.item()
        avg_loss = total_loss / (step + 1)
        train_iter.set_postfix({"avg_loss": f"{avg_loss:.4f}"})

    print(f"âœ… Epoch {epoch + 1} å¹³å‡æŸå¤±: {avg_loss:.4f}")

    # === è¯„ä¼°å¹¶è¾“å‡ºå‡†ç¡®ç‡å’Œç²¾ç¡®ç‡ ===
    acc, pre = compute_metrics(model, tokenizer, train_dataloader, device)
    print(f"ğŸ“Š Epoch {epoch + 1} å‡†ç¡®ç‡ Acc: {acc:.4f}ï¼Œç²¾ç¡®ç‡ Pre: {pre:.4f}")

    # === ä¿å­˜æ¨¡å‹ ===
    save_dir = os.path.join(save_path, f"checkpoint-epoch{epoch + 1}")
    os.makedirs(save_dir, exist_ok=True)
    model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è‡³ï¼š{save_dir}")
