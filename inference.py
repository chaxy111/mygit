import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from tqdm import tqdm
import json

# === å‚æ•°é…ç½® ===
model_dir = "./output/checkpoint-epoch10"  # æ›¿æ¢ä¸ºä½ å®é™…ä¿å­˜æ¨¡å‹çš„è·¯å¾„
input_path = "./data/test1.json"
output_path = "test1_pred.txt"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ å½“å‰ä½¿ç”¨è®¾å¤‡: {device}")

# === åŠ è½½ tokenizer å’Œ æ¨¡å‹ ===
tokenizer = T5Tokenizer.from_pretrained(model_dir)
model = T5ForConditionalGeneration.from_pretrained(model_dir).to(device)
model.eval()

# === è¯»å–æµ‹è¯•æ•°æ® ===
with open(input_path, "r", encoding="utf-8") as f:
    raw_data = json.load(f)

# æ£€æŸ¥æ ¼å¼
assert isinstance(raw_data, list) and "content" in raw_data[0], "âŒ è¾“å…¥ JSON æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘ 'content' å­—æ®µ"

# === æ¨ç†å‡½æ•° ===
def predict(text):
    # ç¼–ç è¾“å…¥æ–‡æœ¬
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=128
    ).to(device)

    # ä½¿ç”¨ AMP è‡ªåŠ¨æ··åˆç²¾åº¦è¿›è¡Œæ¨ç†
    with torch.cuda.amp.autocast():
        output_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_length=256,
            num_beams=4,  # Beam search æé«˜è´¨é‡
            early_stopping=True,
            repetition_penalty=1.2  # æŠ‘åˆ¶é‡å¤
        )

    # è§£ç ç»“æœ
    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return decoded.strip()

# === æ‰§è¡Œæ¨ç† ===
predictions = []
print("ğŸ” å¼€å§‹æ¨ç†...")
for sample in tqdm(raw_data, desc="æ¨ç†ä¸­"):
    content = sample["content"]
    result = predict(content)
    predictions.append(result)

# === å†™å…¥ç»“æœåˆ°æ–‡ä»¶ ===
with open(output_path, "w", encoding="utf-8") as f:
    for line in predictions:
        f.write(line + "\n")

print(f"âœ… æ‰€æœ‰æ ·æœ¬æ¨ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")
